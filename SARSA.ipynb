{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMe1+BXMvpR11JddMCIiVA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mdanash/Machine_learning/blob/main/SARSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym"
      ],
      "metadata": {
        "id": "plSAxcFMWBxD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eps_greedy(Q,s,eps=0.1):\n",
        "  if np.random.uniform(0,1) < eps:\n",
        "    return np.random.randint(Q.shape[1])\n",
        "  else:\n",
        "    return greedy(Q,s)"
      ],
      "metadata": {
        "id": "2HxbAXdWWJ5B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Greedy Policy**\n",
        "\n",
        "\n",
        "> **Returning to maximum state value**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5AWnCFaadTKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy(Q,s):\n",
        "  return np.argmax(Q[s])"
      ],
      "metadata": {
        "id": "6KRo25b1Xx2j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Policy Testing**"
      ],
      "metadata": {
        "id": "nYWBixnUYcvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episodes(env,q,num_episodes=100,to_print=False):\n",
        "  tot_rew=[]\n",
        "  state=env.reset()\n",
        "\n",
        "  for _ in range(num_episodes):\n",
        "    done=False\n",
        "    game_rew=0\n",
        "\n",
        "    while not done:\n",
        "      next_state,rew,done,_=env.step(greedy(Q,state))\n",
        "\n",
        "      state=next_state\n",
        "      game_rew += rew\n",
        "      if done:\n",
        "        state=env.reset()\n",
        "        tot_rew>append(game_rew)\n",
        "      if to_print:\n",
        "        print('Mean score: %3f of %i games!'%(np.mean(tot_rew),num_episodes))\n",
        "\n",
        "    return np.mean(tot_rew)"
      ],
      "metadata": {
        "id": "pNn9EQuaYCSJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SARSA\n",
        "\n",
        "\n",
        "*  Initialize Q matrix\n",
        "*  Decay the epsilon untill it reaches the threshold\n",
        "*  Choose next action\n",
        "*   SARSA update\n",
        "\n",
        "\n",
        "*  Testing the policy\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nSRNH51sgb2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SARSA(env,lr=0.01,num_episodes=10000,eps=0.3,gamma=0.95,eps_decay=0.00005):\n",
        "  nA = env.action_space.n\n",
        "  nS= env.observation_space.n\n",
        "\n",
        "  Q=np.zeros((nS,nA))\n",
        "  games_rewards=[]\n",
        "  test_rewards=[]\n",
        "\n",
        "  for ep in range(num_episodes):\n",
        "    state= env.reset()\n",
        "    done= False\n",
        "    tot_rew= 0\n",
        "\n",
        "  if eps>0.01:\n",
        "    eps -= eps_decay\n",
        "\n",
        "  action= eps_greedy(Q,state,eps)\n",
        "\n",
        "  while not done:\n",
        "    next_state,rew,done,_ =env.step(action)\n",
        "\n",
        "    next_action = eps_greedy(Q,next_state,eps)\n",
        "\n",
        "    Q[state][action] = Q[state][action] + lr*(rew+gamma*Q[next_state][next_action]-Q[state][action])\n",
        "\n",
        "    state = next_state\n",
        "    action = next_action\n",
        "\n",
        "    tot_rew += rew\n",
        "    if done:\n",
        "      games_rewards.append(tot_rew)\n",
        "\n",
        "    if (ep%300) == 0:\n",
        "      test_rew=run_episodes(env,Q,1000)\n",
        "      print(\"EPISODE:{:5d} Eps:{:2.4f} REW:{:2.4f}\".format(ep,eps,test_rew))\n",
        "      test_rewards.append(test_rew)\n",
        "\n",
        "  return Q\n",
        "\n"
      ],
      "metadata": {
        "id": "-A_0WpU2b8GN"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}